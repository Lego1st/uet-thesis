@article{Zhang2019,
author = {Zhang, Xinfeng and Lin, Weisi and Wang, Shiqi and Liu, Jiaying and Ma, Siwei and Gao, Wen},
doi = {10.1109/TIP.2018.2874283},
file = {:home/lego1st/Documents/School/DIPQA/ref/Fine-grained Assessment for Compressed Images.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Image quality assessment,fine-grained distortion levels,image database,peceptual image compression,subjective assessment},
number = {3},
pages = {1163--1175},
publisher = {IEEE},
title = {{Fine-Grained Quality Assessment for Compressed Images}},
volume = {28},
year = {2019}
}
@article{Zhang2011,
abstract = {Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.},
author = {Zhang, Lin and Zhang, Lei and Mou, Xuanqin and Zhang, David},
doi = {10.1109/TIP.2011.2109730},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Gradient,image quality assessment (IQA),low-level feature,phase congruency (PC)},
title = {{FSIM: A feature similarity index for image quality assessment}},
year = {2011}
}
@article{Zhang2017a,
abstract = {{\textcopyright} 2017 IEEE. In video coding, the in-loop filtering has emerged as a key module due to its significant improvement on compression performance since H.264/Advanced Video Coding. Existing incorporated in-loop filters in video coding standards mainly take advantage of the local smoothness prior model used for images. In this paper, we propose a novel adaptive loop filter utilizing image nonlocal prior knowledge by imposing the low-rank constraint on similar image patches for compression noise reduction. In the filtering process, the reconstructed frame is first divided into image patch groups according to image patch similarity. The proposed in-loop filtering is formulated as an optimization problem with low-rank constraint for every group of image patches independently. It can be efficiently solved by soft-thresholding singular values of the matrix composed of image patches in the same group. To adapt the properties of the input sequences and bit budget, an adaptive threshold derivation model is established for every group of image patches according to the characteristics of compressed image patches, quantization parameters, and coding modes. Moreover, frame-level and largest coding unit-level control flags are signaled to further improve the adaptability from the sense of rate-distortion optimization. The performance of the proposed in-loop filter is analyzed when it collaborates with the existing in-loop filters in High Efficiency Video Coding. Extensive experimental results show that our proposed in-loop filter can further improve the performance of state-of-the-art video coding standard significantly, with up to 16{\%} bit-rate savings.},
author = {Zhang, Xinfeng and Xiong, Ruiqin and Lin, Weisi and Zhang, Jian and Wang, Shiqi and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2016.2581618},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {High Efficiency Video Coding (HEVC),in-loop filter,nonlocal,singular value decomposition (SVD),video coding},
title = {{Low-Rank-Based Nonlocal Adaptive Loop Filter for High-Efficiency Video Compression}},
year = {2017}
}
@article{ITU2014,
abstract = {Recommendation ITU-T P.913 describes non-interactive subjective assessment methods for evaluating the one-way overall video quality, audio quality and/or audiovisual quality for applications such as Internet video and distribution quality video. These methods can be used for several different purposes including, but not limited to, comparing the quality of multiple devices, comparing the performance of a device in multiple environments, and subjective assessment where the quality impact of the device and the audiovisual material is confounded.},
author = {ITU},
journal = {Recommendation ITU-T P.913},
keywords = {913,P,P.913},
title = {{Methods for the subjective assessment of video quality, audio quality and audiovisual quality of Internet video and distribution quality television in any environment}},
year = {2014}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v9},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v9},
journal = {ICLR: International Conference on Learning Representations},
title = {{Adam: A method for stochastic gradient descent}},
year = {2015}
}
@inproceedings{Chopra2005,
abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the},
author = {Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
booktitle = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
doi = {10.1109/CVPR.2005.202},
isbn = {0769523722},
title = {{Learning a similarity metric discriminatively, with application to face verification}},
year = {2005}
}
@article{Zhang2014,
abstract = {Perceptual image quality assessment (IQA) aims to use computational models to measure the image quality in consistent with subjective evaluations. Visual saliency (VS) has been widely studied by psychologists, neurobiologists, and computer scientists during the last decade to investigate, which areas of an image will attract the most attention of the human visual system. Intuitively, VS is closely related to IQA in that suprathreshold distortions can largely affect VS maps of images. With this consideration, we propose a simple but very effective full reference IQA method using VS. In our proposed IQA model, the role of VS is twofold. First, VS is used as a feature when computing the local quality map of the distorted image. Second, when pooling the quality score, VS is employed as a weighting function to reflect the importance of a local region. The proposed IQA index is called visual saliency-based index (VSI). Several prominent computational VS models have been investigated in the context of IQA and the best one is chosen for VSI. Extensive experiments performed on four large-scale benchmark databases demonstrate that the proposed IQA index VSI works better in terms of the prediction accuracy than all state-of-the-art IQA indices we can find while maintaining a moderate computational complexity. The MATLAB source code of VSI and the evaluation results are publicly available online at http://sse.tongji.edu.cn/linzhang/IQA/VSI/VSI.htm.},
author = {Zhang, Lin and Shen, Ying and Li, Hongyu},
doi = {10.1109/TIP.2014.2346028},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Perceptual image quality assessment,visual saliency},
title = {{VSI: A visual saliency-induced index for perceptual image quality assessment}},
year = {2014}
}
@article{Sutskever2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Sutskever, Ilya and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan R},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
year = {2014}
}
@article{ITU-TRecommendationP.9102008,
abstract = {SERIES P: TELEPHONE TRANSMISSION QUALITY, TELEPHONE INSTALLATIONS, LOCAL LINE NETWORKS - Audiovisual quality in multimedia services},
author = {{ITU-T Recommendation P.910}},
journal = {International Telecommunication Union, Geneva},
title = {{Subjective video quality assessment methods for multimedia applications}},
year = {2008}
}
@inproceedings{Zhang2010,
abstract = {Image quality assessment (IQA) aims to provide computational models to measure the image quality in a perceptually consistent manner. In this paper, a novel feature based IQA model, namely Riesz-transform based Feature SIMilarity metric (RFSIM), is proposed based on the fact that the human vision system (HVS) perceives an image mainly according to its low-level features. The 1st-order and 2nd-order Riesz transform coefficients of the image are taken as image features, while a feature mask is defined as the edge locations of the image. The similarity index between the reference and distorted images is measured by comparing the two feature maps at key locations marked by the feature mask. Extensive experiments on the comprehensive TID2008 database indicate that the proposed RFSIM metric is more consistent with the subjective evaluation than all the other competing methods evaluated.},
author = {Zhang, Lin and Zhang, Lei and Mou, Xuanqin},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2010.5649275},
isbn = {9781424479948},
issn = {15224880},
keywords = {Image quality assessment,Monogenic signal,Riesz transform},
title = {{RFSIM: A feature based image quality assessment metric using Riesz transforms}},
year = {2010}
}
@inproceedings{Kang2014,
abstract = {In this work we describe a Convolutional Neural Network (CNN) to accurately predict image quality without a reference image. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max and min pooling, two fully connected layers and an output node. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating image quality. This approach achieves state of the art performance on the LIVE dataset and shows excellent generalization ability in cross dataset experiments. Further experiments on images with local distortions demonstrate the local quality estimation ability of our CNN, which is rarely reported in previous literature.},
author = {Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.224},
isbn = {9781479951178},
issn = {10636919},
keywords = {Convolutional Neural Network,image quality assessment},
title = {{Convolutional neural networks for no-reference image quality assessment}},
year = {2014}
}
@inproceedings{Zhang2012,
abstract = {Automatic image quality assessment (IQA) attempts to use computational models to measure the image quality in consistency with subjective ratings. In the past decades, dozens of IQA models have been proposed. Though some of them can predict subjective image quality accurately, their computational costs are usually very high. To meet real-time requirements, in this paper, we propose a novel fast and effective IQA index, namely spectral residual based similarity (SR-SIM), based on a specific visual saliency model, spectral residual visual saliency. SR-SIM is designed based on the hypothesis that an image's visual saliency map is closely related to its perceived quality. Extensive experiments conducted on three large-scale IQA datasets indicate that SR-SIM could achieve better prediction performance than the other state-of-the-art IQA indices evaluated. Moreover, SR-SIM can have a quite low computational complexity. The Matlab source code of SR-SIM and the evaluation results are available online at http://sse.tongji.edu.cn/linzhang/IQA/SR-SIM/SR-SIM.htm.},
author = {Zhang, Lin and Li, Hongyu},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2012.6467149},
isbn = {9781467325332},
issn = {15224880},
keywords = {IQA,spectral residual,visual saliency},
title = {{SR-SIM: A fast and high performance IQA index based on spectral residual}},
year = {2012}
}
@article{Lin2011,
abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
author = {Lin, Weisi and {Jay Kuo}, C. C.},
doi = {10.1016/j.jvcir.2011.01.005},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Common feature and artifact detection,Full reference,Human visual system (HVS),Just-noticeable distortion,No reference,Reduced reference,Signal decomposition,Signal-driven model,Vision-based model,Visual attention},
title = {{Perceptual visual quality metrics: A survey}},
year = {2011}
}
@inproceedings{Zhang2016,
abstract = {During the past few years, there have been various kinds of content-aware image retargeting methods proposed for image resizing. However, the lack of effective objective retargeting quality metric limits the further development of image retargeting. Different from the traditional image quality assessment, the quality degradation of the retargeted images is mainly caused by the geometric changes due to retargeting. In this paper, we propose a practical approach to reveal the geometric changes during image retargeting, and design an Aspect Ratio Similarity (ARS) metric to predict the visual quality of the retargeted image. The experimental results on the widely used dataset show that the proposed metric outperforms the state of the arts.},
author = {Zhang, Yabin and Lin, Weisi and Zhang, Xinfeng and Fang, Yuming and Li, Leida},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2016.7471842},
isbn = {9781479999880},
issn = {15206149},
keywords = {Image retargeting quality assessment,aspect ratio similarity},
title = {{Aspect Ratio Similarity (ARS) for image retargeting quality assessment}},
year = {2016}
}
@inproceedings{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@inproceedings{Ma2016,
abstract = {In-loop filtering has emerged as an essential coding tool since H.264/AVC, due to its delicate design, which reduces different kinds of compression artifacts. However, existing in-loop filters rely only on local image correlations, largely ignoring nonlocal similarities. In this article, the authors explore the design philosophy of in-loop filters and discuss their vision for the future of in-loop filter research by examining the potential of nonlocal similarities. Specifically, the group-based sparse representation, which jointly exploits an image's local and nonlocal self-similarities, lays a novel and meaningful groundwork for in-loop filter design. Hard- and soft-thresholding filtering operations are applied to derive the sparse parameters that are appropriate for compression artifact reduction. Experimental results show that this in-loop filter design can significantly improve the compression performance of the High Efficiency Video Coding (HEVC) standard, leading us in a new direction for improving compression efficiency.},
author = {Ma, Siwei and Zhang, Xinfeng and Zhang, Jian and Jia, Chuanmin and Wang, Shiqi and Gao, Wen},
booktitle = {IEEE Multimedia},
doi = {10.1109/MMUL.2016.16},
issn = {1070986X},
keywords = {HEVC,graphics,in-loop filtering,multimedia,nonlocal similarity,sparse representation,video coding},
title = {{Nonlocal in-loop filter: The way toward next-generation video coding?}},
year = {2016}
}
@article{Ou2011,
abstract = {The quality of video is ultimately judged by human eye; however, mean squared error and the like that have been used as quality metrics are poorly correlated with human perception. Although the characteristics of human visual system have been incorporated into perceptual-based rate control, most existing schemes do not take rate-distortion optimization into consideration. In this paper, we use the structural similarity index as the quality metric for rate-distortion modeling and develop an optimum bit allocation and rate control scheme for video coding. This scheme achieves up to 25{\%} bit-rate reduction over the JM reference software of H.264. Under the rate-distortion optimization framework, the proposed scheme can be easily integrated with the perceptual-based mode decision scheme. The overall bit-rate reduction may reach as high as 32{\%} over the JM reference software.},
author = {Ou, Tao Sheng and Huang, Yi Hsin and Chen, Homer H.},
doi = {10.1109/TCSVT.2011.2129890},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {H.264/AVC,optimum bit allocation,rate control,rate-distortion optimization,structural similarity index},
title = {{SSIM-based perceptual rate control for video coding}},
year = {2011}
}
@article{Wang2018a,
abstract = {{\textcopyright} 1999-2012 IEEE. With the widespread adoption of multidevice communication, such as telecommuting, screen content images (SCIs) have become more closely and frequently related to our daily lives. For SCIs, the tasks of accurate visual quality assessment, high-efficiency compression, and suitable contrast enhancement have thus currently attracted increased attention. In particular, the quality evaluation of SCIs is important due to its good ability for instruction and optimization in various processing systems. Hence, in this paper, we develop a new objective metric for research on perceptual quality assessment of distorted SCIs. Compared to the classical MSE, our method, which mainly relies on simple convolution operators, first highlights the degradations in structures caused by different types of distortions and then detects salient areas where the distortions usually attract more attention. A comparison of our algorithm with the most popular and state-of-The-Art quality measures is performed on two new SCI databases (SIQAD and SCD). Extensive results are provided to verify the superiority and efficiency of the proposed IQA technique.},
author = {Wang, Shiqi and Gu, Ke and Zhang, Xinfeng and Lin, Weisi and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2016.2602764},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Image quality assessment (IQA),reduced reference (RR),screen content images (SCIs)},
title = {{Reduced-Reference Quality Assessment of Screen Content Images}},
year = {2018}
}
@article{Redi2010,
abstract = {Reduced-reference systems can predict in real-time the perceived quality of images for digital broadcasting, only requiring that a limited set of features, extracted from the original undistorted signals, is transmitted together with the image data. This paper uses descriptors based on the color correlogram, analyzing the alterations in the color distribution of an image as a consequence of the occurrence of distortions, for the reduced-reference data. The processing architecture relies on a double layer at the receiver end. The first layer identifies the kind of distortion that may affect the received signal. The second layer deploys a dedicated prediction module for each type of distortion; every predictor yields an objective quality score, thus completing the estimation process. Computational-intelligence models are used extensively to support both layers with empirical training. The double-layer architecture implements a general-purpose image quality assessment system, not being tied up to specific distortions and, at the same time, it allows us to benefit from the accuracy of specific, distortion-targeted metrics. Experimental results based on subjective quality data confirm the general validity of the approach.},
author = {Redi, Judith A. and Gastaldo, Paolo and Heynderickx, Ingrid and Zunino, Rodolfo},
doi = {10.1109/TCSVT.2010.2087456},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Computational intelligence,correlogram,image quality assessment},
title = {{Color distribution information for the reduced-reference assessment of perceived image quality}},
year = {2010}
}
@inproceedings{Wang2006,
abstract = {Many recently proposed perceptual image quality assessment algorithms are implemented in two stages. In the first stage, image quality is evaluated within local regions. This results in a quality/distortion map over the image space. In the second stage, a spatial pooling algorithm is employed that combines the quality/distortion map into a single quality score. While great effort has been devoted to developing algorithms for the first stage, little has been done to find the best strategies for the second stage (and simple spatial average is often used). In this work, we investigate three spatial pooling methods for the second stage: Minkowski pooling, local quality/distortion-weighted pooling, and information content-weighted pooling. Extensive experiments with the LIVE database show that all three methods may improve the prediction performance of perceptual image quality measures, but the third method demonstrates the best potential to be a general and robust method that leads to consistent improvement over a wide range of image distortion types},
author = {Wang, Zhou and Shang, Xinli},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2006.313136},
isbn = {1424404819},
issn = {15224880},
keywords = {Error pooling,Image quality assessment,Information content,Structural similarity,Visual perception},
title = {{Spatial pooling strategies for perceptual image quality assessment}},
year = {2006}
}
@article{Bt2002,
abstract = {Methodology for the subjective assessment of the quality of television pictures. Assessment methodology, environmental conditions etc.},
author = {Bt, Itu-r and Itu-r, Question and Itu, The and Assembly, Radiocommunication},
doi = {http://www.itu.int/rec/R-REC-BT.500/en},
journal = {Methodology},
title = {{RECOMMENDATION ITU-R BT . 500-11 Methodology for the subjective assessment of the quality of television pictures ANNEX 1 Description of assessment methods Common features}},
year = {2002}
}
@article{Shelhamer2017,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2017}
}
@article{Wang2012,
abstract = {We propose a rate-distortion optimization (RDO) scheme based on the structural similarity (SSIM) index, which was found to be a better indicator of perceived image quality than mean-squared error, but has not been fully exploited in the context of image and video coding. At the frame level, an adaptive Lagrange multiplier selection method is proposed based on a novel reduced-reference statistical SSIM estimation algorithm and a rate model that combines the side information with the entropy of the transformed residuals. At the macroblock level, the Lagrange multiplier is further adjusted based on an information theoretical approach that takes into account both the motion information content and perceptual uncertainty of visual speed perception. Finally, the mode for H.264/AVC coding is selected by the SSIM index and the adjusted Lagrange multiplier. Extensive experiments show that the proposed scheme can achieve significantly better rate-SSIM performance and provide better visual quality than conventional RDO coding schemes.},
author = {Wang, Shiqi and Rehman, Abdul and Wang, Zhou and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2011.2168269},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {H.264/AVC coding,Lagrange multiplier,rate-distortion optimization,reduced-reference image quality assessment,structural similarity (SSIM) index},
title = {{SSIM-motivated rate-distortion optimization for video coding}},
year = {2012}
}
@article{Wang2002,
abstract = {We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality{\_}index/demo.html.},
author = {Wang, Zhou and Bovik, Alan C.},
doi = {10.1109/97.995823},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Human visual system (HVS),Image quality measurement,Mean squared error (MSE)},
title = {{A universal image quality index}},
year = {2002}
}
@inproceedings{Wang2018,
abstract = {The just-noticeable-difference (JND) visual perception property has received much attention in characterizing human subjective viewing experience of compressed video. In this work, we quantify the JND-based video quality assessment model using the satisfied user ratio (SUR) curve, and show that the SUR model can be greatly simplified since the JND points of multiple subjects for the same content in the VideoSet can be well modeled by the normal distribution. Then, we design an SUR prediction method with video quality degradation features and masking features and use them to predict the first, second and the third JND points and their corresponding SUR curves. Finally, we verify the performance of the proposed SUR prediction method with different configurations on the VideoSet. The experimental results demonstrate that the proposed SUR prediction method achieves good performance in various resolutions with the mean absolute error (MAE) of the SUR smaller than 0.05 on average.},
author = {Wang, Haiqiang and Zhang, Xinfeng and Yang, Chao and Kuo, C. C.Jay},
booktitle = {2018 Picture Coding Symposium, PCS 2018 - Proceedings},
doi = {10.1109/PCS.2018.8456243},
isbn = {9781538641606},
keywords = {Just Noticeable Difference,Satisfied User Ratio,Video Quality Assessment},
title = {{Analysis and Prediction of JND-Based Video Quality Model}},
year = {2018}
}
@inproceedings{Kang2015,
abstract = {The Iowa Center for Research on Botanical Dietary Supplements seeks$\backslash$nto optimize Echinacea, Hypericum, and Prunella botanical supplements$\backslash$nfor human-health benefit, emphasizing antiviral, anti-inflammatory$\backslash$nand anti-pain activities. This mini-review reports on ongoing studies$\backslash$non Hypericum. The Center uses the genetically diverse, well-documented$\backslash$nHypericum populations collected and maintained at the USDA-ARS North$\backslash$nCentral Regional Plant Introduction Station (NCRPIS), and the strength$\backslash$nof research in synthetic chemistry at Iowa State University to tap$\backslash$nnatural diversity, to help discover key constituents and interactions$\backslash$namong constituents that impact bioactivity and toxicity. The NCRPIS$\backslash$nhas acquired more than 180 distinct populations of Hypericum, with$\backslash$na focus on Hypericum perforatum L. (Hypericaceae), representing about$\backslash$n13{\%} of currently recognized taxa. Center chemists have developed$\backslash$nnovel synthetic pathways for key flavones, acyl phloroglucinols,$\backslash$nhyperolactones and a tetralin that have been found in Hypericum,$\backslash$nand these compounds are used as standards and for bioactivity studies.$\backslash$nBoth light-dependent and light-independent anti-viral activities$\backslash$nhave been identified by using bioactivity-guided fractionation of$\backslash$nH. perforatum and a HIV-1 infection test system. Our Center has focused$\backslash$non light-independent activity, potentially due to novel chemicals,$\backslash$nand polar fractions are undergoing further fractionation. Anti-inflammatory$\backslash$nactivity has been found to be light-independent, and fractionation$\backslash$nof a flavonoid-rich extract revealed four compounds (amentoflavone,$\backslash$nchlorogenic acid, pseudohypericin and quercetin) that interacted$\backslash$nin the light to inhibit lipopolysaccharide-induced prostaglandin$\backslash$nE(2) activity. The Center continues to explore novel populations$\backslash$nof H. perforatum and related species to identify constituents and$\backslash$ninteractions of constituents that contribute to potential health$\backslash$nbenefits related to infection.},
author = {Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2015.7351311},
isbn = {9781479983391},
issn = {15224880},
keywords = {CNN,Image quality assessment,image distortion classification,no-reference},
title = {{Simultaneous estimation of image quality and distortion via multi-task convolutional neural networks}},
year = {2015}
}
@article{Ratnakar2000,
abstract = {—We describe the RD-OPT algorithm forDCTquantization op- timization, which can be used as an efficient tool for near-optimal rate control in DCT-based compression techniques, such as JPEG and MPEG. RD-OPT measures DCT coefficient statistics for the given image data to construct rate/distortion-specific quantization tables with nearly optimal tradeoff},
author = {Ratnakar, Viresh and Livny, Miron},
doi = {10.1109/83.821739},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Discrete cosine transform,JPEG,Lossy image compression,Quantization,Rate control,Rate-distortion tradeoff,Thresholding},
title = {{An efficient algorithm for optimizing DCT quantization}},
year = {2000}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:home/lego1st/Documents/School/DIPQA/ref/1409.1556.pdf:pdf},
pages = {1--14},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{WenHeng;TingtingJiang2017,
author = {{Wen Heng; Tingting Jiang}},
file = {:home/lego1st/Documents/School/DIPQA/ref/Image-Patch.pdf:pdf},
isbn = {9781509041176},
journal = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2017},
pages = {1238--1242},
title = {{FROM IMAGE QUALITY TO PATCH QUALITY : AN IMAGE-PATCH MODEL FOR NO-REFERENCE IMAGE QUALITY ASSESSMENT Wen Heng and Tingting Jiang National Engineering Laboratory for Video Technology , Cooperative Medianet Innovation Center , School of Electronics Engineer}},
year = {2017}
}
@article{Chandler2013,
abstract = {Image quality assessment (IQA) has been a topic of intense research over the last several decades. With each year comes an increasing number of new IQA algorithms, extensions of existing IQA algorithms, and applications of IQA to other disciplines. In this article, I first provide an up-to-date review of research in IQA, and then I highlight several open challenges in this field. The first half of this article provides discuss key properties of visual perception, image quality databases, existing full-reference, no-reference, and reduced-reference IQA algorithms. Yet, despite the remarkable progress that has been made in IQA, many fundamental challenges remain largely unsolved. The second half of this article highlights some of these challenges. I specifically discuss challenges related to lack of complete perceptual models for: natural images, compound and suprathreshold distortions, and multiple distortions, and the interactive effects of these distortions on the images. I also discuss challenges related to IQA of images containing nontraditional, and I discuss challenges related to the computational efficiency. The goal of this article is not only to help practitioners and researchers keep abreast of the recent advances in IQA, but to also raise awareness of the key limitations of current IQA knowledge.},
author = {Chandler, Damon M.},
doi = {10.1155/2013/905685},
journal = {ISRN Signal Processing},
title = {{Seven Challenges in Image Quality Assessment: Past, Present, and Future Research}},
year = {2013}
}
@article{Bosse2018,
abstract = {We present a deep neural network-based approach to image quality assessment (IQA). The network is trained end-to-end and comprises ten convolutional layers and five pooling layers for feature extraction, and two fully connected layers for regression, which makes it significantly deeper than related IQA models. Unique features of the proposed architecture are that: 1) with slight adaptations it can be used in a no-reference (NR) as well as in a full-reference (FR) IQA setting and 2) it allows for joint learning of local quality and local weights, i.e., relative importance of local quality to the global quality estimate, in an unified framework. Our approach is purely data-driven and does not rely on hand-crafted features or other types of prior domain knowledge about the human visual system or image statistics. We evaluate the proposed approach on the LIVE, CISQ, and TID2013 databases as well as the LIVE In the wild image quality challenge database and show superior performance to state-of-the-art NR and FR IQA methods. Finally, cross-database evaluation shows a high ability to generalize between different databases, indicating a high robustness of the learned features.},
archivePrefix = {arXiv},
arxivId = {1612.01697},
author = {Bosse, Sebastian and Maniry, Dominique and M{\"{u}}ller, Klaus Robert and Wiegand, Thomas and Samek, Wojciech},
doi = {10.1109/TIP.2017.2760518},
eprint = {1612.01697},
file = {:home/lego1st/Documents/School/DIPQA/ref/DNNs-for-NR-and-FR-IQA.pdf:pdf},
isbn = {9781509059669},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Full-reference image quality assessment,deep learning,feature extraction,neural networks,no-reference image quality assessment,quality pooling,regression},
number = {1},
pages = {206--219},
pmid = {24478645},
title = {{Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment}},
volume = {27},
year = {2018}
}
@inproceedings{Ye2012,
abstract = {In this paper, we present an efficient general-purpose objective $\backslash$nno-reference (NR) image quality assessment (IQA) framework based on unsupervised $\backslash$nfeature learning. The goal is to build a computational model to automatically $\backslash$npredict human perceived image quality without a reference image and without $\backslash$nknowing the distortion present in the image. Previous approaches for this $\backslash$nproblem typically rely on hand-crafted features which are carefully designed $\backslash$nbased on prior knowledge. In contrast, we use raw-image-patches extracted from a $\backslash$nset of unlabeled images to learn a dictionary in an unsupervised manner. We use $\backslash$nsoft-assignment coding with max pooling to obtain effective image $\backslash$nrepresentations for quality estimation. The proposed algorithm is very $\backslash$ncomputationally appealing, using raw image patches as local descriptors and $\backslash$nusing soft-assignment for encoding. Furthermore, unlike previous methods, our $\backslash$nunsupervised feature learning strategy enables our method to adapt to different $\backslash$ndomains. CORNIA (Codebook Representation for No-Reference Image Assessment) is $\backslash$ntested on LIVE database and shown to perform statistically better than the $\backslash$nfull-reference quality measure, structural similarity index (SSIM) and is shown $\backslash$nto be comparable to state-of-the-art general purpose NR-IQA algorithms.},
author = {Ye, Peng and Kumar, Jayant and Kang, Le and Doermann, David},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247789},
isbn = {9781467312264},
issn = {10636919},
title = {{Unsupervised feature learning framework for no-reference image quality assessment}},
year = {2012}
}
@article{BROMLEY2004,
abstract = {This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a "Siamese" neural network. This network consists of two identical sub-networks joined at their out-puts. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance be-tween the two feature vectors. Verification consists of comparing an extracted feature vector {\~{}}ith a stored feature vector for the signer. Signatures closer to this stored representation than a chosen thresh-old are accepted, all other signatures are rejected as forgeries.},
author = {BROMLEY, JANE and BENTZ, JAMES W. and BOTTOU, L{\'{E}}ON and GUYON, ISABELLE and LECUN, YANN and MOORE, CLIFF and S{\"{A}}CKINGER, EDUARD and SHAH, ROOPAK},
doi = {10.1142/s0218001493000339},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
title = {{SIGNATURE VERIFICATION USING A “SIAMESE” TIME DELAY NEURAL NETWORK}},
year = {2004}
}
@article{Zhang2017,
author = {Zhang, Xinfeng and Wang, Shiqi and Gu, Ke and Lin, Weisi and Ma, Siwei and Gao, Wen},
doi = {10.1109/LSP.2016.2641456},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Image compression,just-noticeable difference (JND),quantization table,rate-distortion},
title = {{Just-Noticeable Difference-Based Perceptual Optimization for JPEG Compression}},
year = {2017}
}
@article{Atsawaraungsuk2015,
abstract = {Providing a satisfactory visual experience is one of the main goals for present-day electronic multimedia devices. All the enabling technologies for storage, transmission, compression, rendering should preserve, and possibly enhance, the quality of the video signal; to do so, quality control mechanisms are required. These mechanisms rely on systems that can assess the visual quality of the incoming signal consistently with human perception. Computational Intelligence (CI) paradigms represent a suitable technology to tackle this challenging problem. The present research introduces an augmented version of the basic Extreme Learning Machine (ELM), the Circular-ELM (C-ELM), which proves effective in addressing the visual quality assessment problem. The C-ELM model derives from the original Circular BackPropagation (CBP) architecture, in which the input vector of a conventional MultiLayer Perceptron (MLP) is augmented by one additional dimension, the circular input; this paper shows that C-ELM can actually benefit from the enhancement provided by the circular input without losing any of the fruitful properties that characterize the basic ELM framework. In the proposed framework, C-ELM handles the actual mapping of visual signals into quality scores, successfully reproducing perceptual mechanisms. Its effectiveness is proved on recognized benchmarks and for four different types of distortions. {\textcopyright} 2012 Elsevier B.V.},
author = {Atsawaraungsuk, Sarutte and Horata, Punyaphol},
doi = {10.1007/978-3-662-46578-3_77},
issn = {18761119},
journal = {Lecture Notes in Electrical Engineering},
title = {{Evolutionary circular-ELM for the reduced-reference assessment of perceived image quality}},
year = {2015}
}
@article{Wallace1992,
abstract = {A joint ISO/CCITT committee known as JPEG (Joint Photographic$\backslash$nExperts Group) has been working to establish the first international$\backslash$ncompression standard for continuous-tone still images, both grayscale$\backslash$nand color. JPEG's proposed standard aims to be generic, to support a$\backslash$nwide variety of applications for continuous-tone images. To meet the$\backslash$ndiffering needs of many applications, the JPEG standard includes two$\backslash$nbasic compression methods, each with various modes of operation. A DCT$\backslash$n(discrete cosine transform)-based method is specified for `lossy'$\backslash$ncompression, and a predictive method for `lossless' compression. JPEG$\backslash$nfeatures a simple lossy technique known as the Baseline method, a subset$\backslash$nof the other DCT-based modes of operation. The Baseline method has been$\backslash$nby far the most widely implemented JPEG method to date, and is$\backslash$nsufficient in its own right for a large number of applications. The$\backslash$nauthor provides an overview of the JPEG standard, and focuses in detail$\backslash$non the Baseline method},
author = {Wallace, Gregory K.},
doi = {10.1109/30.125072},
issn = {00983063},
journal = {IEEE Transactions on Consumer Electronics},
title = {{The JPEG still picture compression standard}},
year = {1992}
}
@article{Channappayya2008,
abstract = {In this paper, we derive bounds on the structural similarity (SSIM) index as a function of quantization rate for fixed-rate uniform quantization of image discrete cosine transform (DCT) coefficients under the high-rate assumption. The space domain SSIM index is first expressed in terms of the DCT coefficients of the space domain vectors. The transform domain SSIM index is then used to derive bounds on the average SSIM index as a function of quantization rate for uniform, Gaussian, and Laplacian sources. As an illustrative example, uniform quantization of the DCT coefficients of natural images is considered. We show that the SSIM index between the reference and quantized images fall within the bounds for a large set of natural images. Further, we show using a simple example that the proposed bounds could be very useful for rate allocation problems in practical image and video coding applications.},
author = {Channappayya, Sumohana S. and Bovik, Alan Conrad and Heath, Robert W.},
doi = {10.1109/TIP.2008.2001400},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
title = {{Rate bounds on SSIM index of quantized images}},
year = {2008}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
author = {Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.169},
isbn = {9781467383912},
issn = {15505499},
title = {{Fast R-CNN}},
year = {2015}
}
@article{Huynh-Thu2008,
abstract = {Experimental data are presented that clearly demonstrate the scope of application of peak signal-to-noise ratio (PSNR) as a video quality metric. It is shown that as long as the video content and the codec type are not changed, PSNR is a valid quality measure. However, when the content is changed, correlation between subjective quality and PSNR is highly reduced. Hence PSNR cannot be a reliable method for assessing the video quality across different video contents},
author = {Huynh-Thu, Q. and Ghanbari, M.},
doi = {10.1049/el:20080522},
issn = {00135194},
journal = {Electronics Letters},
title = {{Scope of validity of PSNR in image/video quality assessment}},
year = {2008}
}
@inproceedings{Chen2010,
abstract = {Investigation on the human perception can play an important role in video signal processing. Recently, there has been great interest in incorporating the human perception in video coding systems to enhance the perceptual quality of the represented visual signal. However, the limited understanding of the human visual system and high complexity of computational models of human visual system make it a challenging task. Furthermore, the hybrid video coding structure brings difficulties to integrate computational models with coding components to fulfill the requirements. In this paper, we review the physiological characteristics of human perception and address the most relevant aspects to video coding applications. Moreover, we discuss the computational models and metrics which guide the design and implementation of the video coding system, as well as the recent advances in perceptual video coding. To introduce this overview with the latest technologies and most promising directions in perceptual video coding, we focus on three key areas. Specifically, we cover 1) visual attention and sensitivity modeling, with which we concentrate on the computational models of bottom-up and top-down attention, contrast sensitivity functions and masking effects, and fovea based manipulations; 2) perceptual quality optimization for constrained video coding, with which we discuss how to achieve maximum perceptual quality whilst satisfying various constraints; and 3) the impact of the human perception on advanced video applications, including emerging immersive multimedia services, and compression of high dynamic range video content and 3D video. For each aspect, we discuss the major challenges, highlight significant approaches, and outline future research directions.},
author = {Chen, Zhenzhong and Lin, Weisi and {Ngi Ngan}, King},
booktitle = {2010 IEEE International Conference on Multimedia and Expo, ICME 2010},
doi = {10.1109/ICME.2010.5582549},
isbn = {9781424474912},
keywords = {Human visual system,Perceptual video coding,Quality optimization,Visual perception},
title = {{Perceptual video coding: Challenges and approaches}},
year = {2010}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
title = {{Image quality assessment: From error visibility to structural similarity}},
year = {2004}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
title = {{A logical calculus of the ideas immanent in nervous activity}},
year = {1943}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
issn = {00280836},
journal = {Nature},
title = {{Learning representations by back-propagating errors}},
year = {1986}
}
@article{Rowley1998,
abstract = {We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates},
author = {Rowley, Henry A. and Baluja, Shumeet and Kanade, Takeo},
doi = {10.1109/34.655647},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Artificial neural networks,Computer vision,Face detection, pattern recognition,Machine learning},
title = {{Neural network-based face detection}},
year = {1998}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
title = {{Gradient-based learning applied to document recognition}},
year = {1998}
}
@inproceedings{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {ImageNet Classification with Deep Convolutional Neural Networks},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@inproceedings{Scherer2010,
abstract = {A common practice to gain invariant features in object recog- nition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant proper- ties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57{\%} on the NORB normalized-uniform dataset and 5.6{\%} on the NORB jittered-cluttered dataset.},
author = {Scherer, Dominik and M{\"{u}}ller, Andreas and Behnke, Sven},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15825-4_10},
isbn = {3642158242},
issn = {03029743},
title = {{Evaluation of pooling operations in convolutional architectures for object recognition}},
year = {2010}
}
@article{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krizhevsky, Alex},
doi = {10.1.1.222.9220},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Technical Report, Department of Computer Science University of Toronto},
pmid = {25246403},
title = {{Learning Multiple Layers of Features from Tiny Images}},
year = {2009}
}
