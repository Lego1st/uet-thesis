@inproceedings{Chen2010,
abstract = {Investigation on the human perception can play an important role in video signal processing. Recently, there has been great interest in incorporating the human perception in video coding systems to enhance the perceptual quality of the represented visual signal. However, the limited understanding of the human visual system and high complexity of computational models of human visual system make it a challenging task. Furthermore, the hybrid video coding structure brings difficulties to integrate computational models with coding components to fulfill the requirements. In this paper, we review the physiological characteristics of human perception and address the most relevant aspects to video coding applications. Moreover, we discuss the computational models and metrics which guide the design and implementation of the video coding system, as well as the recent advances in perceptual video coding. To introduce this overview with the latest technologies and most promising directions in perceptual video coding, we focus on three key areas. Specifically, we cover 1) visual attention and sensitivity modeling, with which we concentrate on the computational models of bottom-up and top-down attention, contrast sensitivity functions and masking effects, and fovea based manipulations; 2) perceptual quality optimization for constrained video coding, with which we discuss how to achieve maximum perceptual quality whilst satisfying various constraints; and 3) the impact of the human perception on advanced video applications, including emerging immersive multimedia services, and compression of high dynamic range video content and 3D video. For each aspect, we discuss the major challenges, highlight significant approaches, and outline future research directions.},
author = {Chen, Zhenzhong and Lin, Weisi and {Ngi Ngan}, King},
booktitle = {2010 IEEE International Conference on Multimedia and Expo, ICME 2010},
doi = {10.1109/ICME.2010.5582549},
isbn = {9781424474912},
keywords = {Human visual system,Perceptual video coding,Quality optimization,Visual perception},
title = {{Perceptual video coding: Challenges and approaches}},
year = {2010}
}
@article{Wang2018a,
abstract = {{\textcopyright} 1999-2012 IEEE. With the widespread adoption of multidevice communication, such as telecommuting, screen content images (SCIs) have become more closely and frequently related to our daily lives. For SCIs, the tasks of accurate visual quality assessment, high-efficiency compression, and suitable contrast enhancement have thus currently attracted increased attention. In particular, the quality evaluation of SCIs is important due to its good ability for instruction and optimization in various processing systems. Hence, in this paper, we develop a new objective metric for research on perceptual quality assessment of distorted SCIs. Compared to the classical MSE, our method, which mainly relies on simple convolution operators, first highlights the degradations in structures caused by different types of distortions and then detects salient areas where the distortions usually attract more attention. A comparison of our algorithm with the most popular and state-of-The-Art quality measures is performed on two new SCI databases (SIQAD and SCD). Extensive results are provided to verify the superiority and efficiency of the proposed IQA technique.},
author = {Wang, Shiqi and Gu, Ke and Zhang, Xinfeng and Lin, Weisi and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2016.2602764},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Image quality assessment (IQA),reduced reference (RR),screen content images (SCIs)},
title = {{Reduced-Reference Quality Assessment of Screen Content Images}},
year = {2018}
}
@inproceedings{Zhang2016,
abstract = {During the past few years, there have been various kinds of content-aware image retargeting methods proposed for image resizing. However, the lack of effective objective retargeting quality metric limits the further development of image retargeting. Different from the traditional image quality assessment, the quality degradation of the retargeted images is mainly caused by the geometric changes due to retargeting. In this paper, we propose a practical approach to reveal the geometric changes during image retargeting, and design an Aspect Ratio Similarity (ARS) metric to predict the visual quality of the retargeted image. The experimental results on the widely used dataset show that the proposed metric outperforms the state of the arts.},
author = {Zhang, Yabin and Lin, Weisi and Zhang, Xinfeng and Fang, Yuming and Li, Leida},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2016.7471842},
isbn = {9781479999880},
issn = {15206149},
keywords = {Image retargeting quality assessment,aspect ratio similarity},
title = {{Aspect Ratio Similarity (ARS) for image retargeting quality assessment}},
year = {2016}
}
@article{Zhang2014,
abstract = {Perceptual image quality assessment (IQA) aims to use computational models to measure the image quality in consistent with subjective evaluations. Visual saliency (VS) has been widely studied by psychologists, neurobiologists, and computer scientists during the last decade to investigate, which areas of an image will attract the most attention of the human visual system. Intuitively, VS is closely related to IQA in that suprathreshold distortions can largely affect VS maps of images. With this consideration, we propose a simple but very effective full reference IQA method using VS. In our proposed IQA model, the role of VS is twofold. First, VS is used as a feature when computing the local quality map of the distorted image. Second, when pooling the quality score, VS is employed as a weighting function to reflect the importance of a local region. The proposed IQA index is called visual saliency-based index (VSI). Several prominent computational VS models have been investigated in the context of IQA and the best one is chosen for VSI. Extensive experiments performed on four large-scale benchmark databases demonstrate that the proposed IQA index VSI works better in terms of the prediction accuracy than all state-of-the-art IQA indices we can find while maintaining a moderate computational complexity. The MATLAB source code of VSI and the evaluation results are publicly available online at http://sse.tongji.edu.cn/linzhang/IQA/VSI/VSI.htm.},
author = {Zhang, Lin and Shen, Ying and Li, Hongyu},
doi = {10.1109/TIP.2014.2346028},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Perceptual image quality assessment,visual saliency},
title = {{VSI: A visual saliency-induced index for perceptual image quality assessment}},
year = {2014}
}
@article{Bt2002,
abstract = {Methodology for the subjective assessment of the quality of television pictures. Assessment methodology, environmental conditions etc.},
author = {Bt, Itu-r and Itu-r, Question and Itu, The and Assembly, Radiocommunication},
doi = {http://www.itu.int/rec/R-REC-BT.500/en},
journal = {Methodology},
title = {{RECOMMENDATION ITU-R BT . 500-11 Methodology for the subjective assessment of the quality of television pictures ANNEX 1 Description of assessment methods Common features}},
year = {2002}
}
@article{Huynh-Thu2008,
abstract = {Experimental data are presented that clearly demonstrate the scope of application of peak signal-to-noise ratio (PSNR) as a video quality metric. It is shown that as long as the video content and the codec type are not changed, PSNR is a valid quality measure. However, when the content is changed, correlation between subjective quality and PSNR is highly reduced. Hence PSNR cannot be a reliable method for assessing the video quality across different video contents},
author = {Huynh-Thu, Q. and Ghanbari, M.},
doi = {10.1049/el:20080522},
issn = {00135194},
journal = {Electronics Letters},
title = {{Scope of validity of PSNR in image/video quality assessment}},
year = {2008}
}
@article{Zhang2017,
author = {Zhang, Xinfeng and Wang, Shiqi and Gu, Ke and Lin, Weisi and Ma, Siwei and Gao, Wen},
doi = {10.1109/LSP.2016.2641456},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Image compression,just-noticeable difference (JND),quantization table,rate-distortion},
title = {{Just-Noticeable Difference-Based Perceptual Optimization for JPEG Compression}},
year = {2017}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v9},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v9},
journal = {ICLR: International Conference on Learning Representations},
title = {{Adam: A method for stochastic gradient descent}},
year = {2015}
}
@article{Sutskever2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Sutskever, Ilya and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan R},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
year = {2014}
}
@inproceedings{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey},
booktitle = {Proceedings of the 27th International Conference on Machine Learning},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Shelhamer2017,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2017}
}
@inproceedings{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
author = {Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.169},
isbn = {9781467383912},
issn = {15505499},
title = {{Fast R-CNN}},
year = {2015}
}
@article{Wang2002,
abstract = {We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality{\_}index/demo.html.},
author = {Wang, Zhou and Bovik, Alan C.},
doi = {10.1109/97.995823},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Human visual system (HVS),Image quality measurement,Mean squared error (MSE)},
title = {{A universal image quality index}},
year = {2002}
}
@inproceedings{Zhang2012,
abstract = {Automatic image quality assessment (IQA) attempts to use computational models to measure the image quality in consistency with subjective ratings. In the past decades, dozens of IQA models have been proposed. Though some of them can predict subjective image quality accurately, their computational costs are usually very high. To meet real-time requirements, in this paper, we propose a novel fast and effective IQA index, namely spectral residual based similarity (SR-SIM), based on a specific visual saliency model, spectral residual visual saliency. SR-SIM is designed based on the hypothesis that an image's visual saliency map is closely related to its perceived quality. Extensive experiments conducted on three large-scale IQA datasets indicate that SR-SIM could achieve better prediction performance than the other state-of-the-art IQA indices evaluated. Moreover, SR-SIM can have a quite low computational complexity. The Matlab source code of SR-SIM and the evaluation results are available online at http://sse.tongji.edu.cn/linzhang/IQA/SR-SIM/SR-SIM.htm.},
author = {Zhang, Lin and Li, Hongyu},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2012.6467149},
isbn = {9781467325332},
issn = {15224880},
keywords = {IQA,spectral residual,visual saliency},
title = {{SR-SIM: A fast and high performance IQA index based on spectral residual}},
year = {2012}
}
@inproceedings{Chopra2005,
abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the},
author = {Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
booktitle = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
doi = {10.1109/CVPR.2005.202},
isbn = {0769523722},
title = {{Learning a similarity metric discriminatively, with application to face verification}},
year = {2005}
}
@article{BROMLEY2004,
abstract = {This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a "Siamese" neural network. This network consists of two identical sub-networks joined at their out-puts. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance be-tween the two feature vectors. Verification consists of comparing an extracted feature vector {\~{}}ith a stored feature vector for the signer. Signatures closer to this stored representation than a chosen thresh-old are accepted, all other signatures are rejected as forgeries.},
author = {BROMLEY, JANE and BENTZ, JAMES W. and BOTTOU, L{\'{E}}ON and GUYON, ISABELLE and LECUN, YANN and MOORE, CLIFF and S{\"{A}}CKINGER, EDUARD and SHAH, ROOPAK},
doi = {10.1142/s0218001493000339},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
title = {{SIGNATURE VERIFICATION USING A “SIAMESE” TIME DELAY NEURAL NETWORK}},
year = {2004}
}
@inproceedings{Ye2012,
abstract = {In this paper, we present an efficient general-purpose objective $\backslash$nno-reference (NR) image quality assessment (IQA) framework based on unsupervised $\backslash$nfeature learning. The goal is to build a computational model to automatically $\backslash$npredict human perceived image quality without a reference image and without $\backslash$nknowing the distortion present in the image. Previous approaches for this $\backslash$nproblem typically rely on hand-crafted features which are carefully designed $\backslash$nbased on prior knowledge. In contrast, we use raw-image-patches extracted from a $\backslash$nset of unlabeled images to learn a dictionary in an unsupervised manner. We use $\backslash$nsoft-assignment coding with max pooling to obtain effective image $\backslash$nrepresentations for quality estimation. The proposed algorithm is very $\backslash$ncomputationally appealing, using raw image patches as local descriptors and $\backslash$nusing soft-assignment for encoding. Furthermore, unlike previous methods, our $\backslash$nunsupervised feature learning strategy enables our method to adapt to different $\backslash$ndomains. CORNIA (Codebook Representation for No-Reference Image Assessment) is $\backslash$ntested on LIVE database and shown to perform statistically better than the $\backslash$nfull-reference quality measure, structural similarity index (SSIM) and is shown $\backslash$nto be comparable to state-of-the-art general purpose NR-IQA algorithms.},
author = {Ye, Peng and Kumar, Jayant and Kang, Le and Doermann, David},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247789},
isbn = {9781467312264},
issn = {10636919},
title = {{Unsupervised feature learning framework for no-reference image quality assessment}},
year = {2012}
}
@inproceedings{Kang2014,
abstract = {In this work we describe a Convolutional Neural Network (CNN) to accurately predict image quality without a reference image. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max and min pooling, two fully connected layers and an output node. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating image quality. This approach achieves state of the art performance on the LIVE dataset and shows excellent generalization ability in cross dataset experiments. Further experiments on images with local distortions demonstrate the local quality estimation ability of our CNN, which is rarely reported in previous literature.},
author = {Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.224},
isbn = {9781479951178},
issn = {10636919},
keywords = {Convolutional Neural Network,image quality assessment},
title = {{Convolutional neural networks for no-reference image quality assessment}},
year = {2014}
}
@inproceedings{Kang2015,
abstract = {The Iowa Center for Research on Botanical Dietary Supplements seeks$\backslash$nto optimize Echinacea, Hypericum, and Prunella botanical supplements$\backslash$nfor human-health benefit, emphasizing antiviral, anti-inflammatory$\backslash$nand anti-pain activities. This mini-review reports on ongoing studies$\backslash$non Hypericum. The Center uses the genetically diverse, well-documented$\backslash$nHypericum populations collected and maintained at the USDA-ARS North$\backslash$nCentral Regional Plant Introduction Station (NCRPIS), and the strength$\backslash$nof research in synthetic chemistry at Iowa State University to tap$\backslash$nnatural diversity, to help discover key constituents and interactions$\backslash$namong constituents that impact bioactivity and toxicity. The NCRPIS$\backslash$nhas acquired more than 180 distinct populations of Hypericum, with$\backslash$na focus on Hypericum perforatum L. (Hypericaceae), representing about$\backslash$n13{\%} of currently recognized taxa. Center chemists have developed$\backslash$nnovel synthetic pathways for key flavones, acyl phloroglucinols,$\backslash$nhyperolactones and a tetralin that have been found in Hypericum,$\backslash$nand these compounds are used as standards and for bioactivity studies.$\backslash$nBoth light-dependent and light-independent anti-viral activities$\backslash$nhave been identified by using bioactivity-guided fractionation of$\backslash$nH. perforatum and a HIV-1 infection test system. Our Center has focused$\backslash$non light-independent activity, potentially due to novel chemicals,$\backslash$nand polar fractions are undergoing further fractionation. Anti-inflammatory$\backslash$nactivity has been found to be light-independent, and fractionation$\backslash$nof a flavonoid-rich extract revealed four compounds (amentoflavone,$\backslash$nchlorogenic acid, pseudohypericin and quercetin) that interacted$\backslash$nin the light to inhibit lipopolysaccharide-induced prostaglandin$\backslash$nE(2) activity. The Center continues to explore novel populations$\backslash$nof H. perforatum and related species to identify constituents and$\backslash$ninteractions of constituents that contribute to potential health$\backslash$nbenefits related to infection.},
author = {Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2015.7351311},
isbn = {9781479983391},
issn = {15224880},
keywords = {CNN,Image quality assessment,image distortion classification,no-reference},
title = {{Simultaneous estimation of image quality and distortion via multi-task convolutional neural networks}},
year = {2015}
}
@article{Zhang2011,
abstract = {Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.},
author = {Zhang, Lin and Zhang, Lei and Mou, Xuanqin and Zhang, David},
doi = {10.1109/TIP.2011.2109730},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Gradient,image quality assessment (IQA),low-level feature,phase congruency (PC)},
title = {{FSIM: A feature similarity index for image quality assessment}},
year = {2011}
}
@inproceedings{Zhang2010,
abstract = {Image quality assessment (IQA) aims to provide computational models to measure the image quality in a perceptually consistent manner. In this paper, a novel feature based IQA model, namely Riesz-transform based Feature SIMilarity metric (RFSIM), is proposed based on the fact that the human vision system (HVS) perceives an image mainly according to its low-level features. The 1st-order and 2nd-order Riesz transform coefficients of the image are taken as image features, while a feature mask is defined as the edge locations of the image. The similarity index between the reference and distorted images is measured by comparing the two feature maps at key locations marked by the feature mask. Extensive experiments on the comprehensive TID2008 database indicate that the proposed RFSIM metric is more consistent with the subjective evaluation than all the other competing methods evaluated.},
author = {Zhang, Lin and Zhang, Lei and Mou, Xuanqin},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2010.5649275},
isbn = {9781424479948},
issn = {15224880},
keywords = {Image quality assessment,Monogenic signal,Riesz transform},
title = {{RFSIM: A feature based image quality assessment metric using Riesz transforms}},
year = {2010}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
title = {{Image quality assessment: From error visibility to structural similarity}},
year = {2004}
}
@inproceedings{Ma2016,
abstract = {In-loop filtering has emerged as an essential coding tool since H.264/AVC, due to its delicate design, which reduces different kinds of compression artifacts. However, existing in-loop filters rely only on local image correlations, largely ignoring nonlocal similarities. In this article, the authors explore the design philosophy of in-loop filters and discuss their vision for the future of in-loop filter research by examining the potential of nonlocal similarities. Specifically, the group-based sparse representation, which jointly exploits an image's local and nonlocal self-similarities, lays a novel and meaningful groundwork for in-loop filter design. Hard- and soft-thresholding filtering operations are applied to derive the sparse parameters that are appropriate for compression artifact reduction. Experimental results show that this in-loop filter design can significantly improve the compression performance of the High Efficiency Video Coding (HEVC) standard, leading us in a new direction for improving compression efficiency.},
author = {Ma, Siwei and Zhang, Xinfeng and Zhang, Jian and Jia, Chuanmin and Wang, Shiqi and Gao, Wen},
booktitle = {IEEE Multimedia},
doi = {10.1109/MMUL.2016.16},
issn = {1070986X},
keywords = {HEVC,graphics,in-loop filtering,multimedia,nonlocal similarity,sparse representation,video coding},
title = {{Nonlocal in-loop filter: The way toward next-generation video coding?}},
year = {2016}
}
@article{Zhang2017a,
abstract = {{\textcopyright} 2017 IEEE. In video coding, the in-loop filtering has emerged as a key module due to its significant improvement on compression performance since H.264/Advanced Video Coding. Existing incorporated in-loop filters in video coding standards mainly take advantage of the local smoothness prior model used for images. In this paper, we propose a novel adaptive loop filter utilizing image nonlocal prior knowledge by imposing the low-rank constraint on similar image patches for compression noise reduction. In the filtering process, the reconstructed frame is first divided into image patch groups according to image patch similarity. The proposed in-loop filtering is formulated as an optimization problem with low-rank constraint for every group of image patches independently. It can be efficiently solved by soft-thresholding singular values of the matrix composed of image patches in the same group. To adapt the properties of the input sequences and bit budget, an adaptive threshold derivation model is established for every group of image patches according to the characteristics of compressed image patches, quantization parameters, and coding modes. Moreover, frame-level and largest coding unit-level control flags are signaled to further improve the adaptability from the sense of rate-distortion optimization. The performance of the proposed in-loop filter is analyzed when it collaborates with the existing in-loop filters in High Efficiency Video Coding. Extensive experimental results show that our proposed in-loop filter can further improve the performance of state-of-the-art video coding standard significantly, with up to 16{\%} bit-rate savings.},
author = {Zhang, Xinfeng and Xiong, Ruiqin and Lin, Weisi and Zhang, Jian and Wang, Shiqi and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2016.2581618},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {High Efficiency Video Coding (HEVC),in-loop filter,nonlocal,singular value decomposition (SVD),video coding},
title = {{Low-Rank-Based Nonlocal Adaptive Loop Filter for High-Efficiency Video Compression}},
year = {2017}
}
@article{Wang2012,
abstract = {We propose a rate-distortion optimization (RDO) scheme based on the structural similarity (SSIM) index, which was found to be a better indicator of perceived image quality than mean-squared error, but has not been fully exploited in the context of image and video coding. At the frame level, an adaptive Lagrange multiplier selection method is proposed based on a novel reduced-reference statistical SSIM estimation algorithm and a rate model that combines the side information with the entropy of the transformed residuals. At the macroblock level, the Lagrange multiplier is further adjusted based on an information theoretical approach that takes into account both the motion information content and perceptual uncertainty of visual speed perception. Finally, the mode for H.264/AVC coding is selected by the SSIM index and the adjusted Lagrange multiplier. Extensive experiments show that the proposed scheme can achieve significantly better rate-SSIM performance and provide better visual quality than conventional RDO coding schemes.},
author = {Wang, Shiqi and Rehman, Abdul and Wang, Zhou and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2011.2168269},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {H.264/AVC coding,Lagrange multiplier,rate-distortion optimization,reduced-reference image quality assessment,structural similarity (SSIM) index},
title = {{SSIM-motivated rate-distortion optimization for video coding}},
year = {2012}
}
@article{Channappayya2008,
abstract = {In this paper, we derive bounds on the structural similarity (SSIM) index as a function of quantization rate for fixed-rate uniform quantization of image discrete cosine transform (DCT) coefficients under the high-rate assumption. The space domain SSIM index is first expressed in terms of the DCT coefficients of the space domain vectors. The transform domain SSIM index is then used to derive bounds on the average SSIM index as a function of quantization rate for uniform, Gaussian, and Laplacian sources. As an illustrative example, uniform quantization of the DCT coefficients of natural images is considered. We show that the SSIM index between the reference and quantized images fall within the bounds for a large set of natural images. Further, we show using a simple example that the proposed bounds could be very useful for rate allocation problems in practical image and video coding applications.},
author = {Channappayya, Sumohana S. and Bovik, Alan Conrad and Heath, Robert W.},
doi = {10.1109/TIP.2008.2001400},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
title = {{Rate bounds on SSIM index of quantized images}},
year = {2008}
}
@inproceedings{Wang2018,
abstract = {The just-noticeable-difference (JND) visual perception property has received much attention in characterizing human subjective viewing experience of compressed video. In this work, we quantify the JND-based video quality assessment model using the satisfied user ratio (SUR) curve, and show that the SUR model can be greatly simplified since the JND points of multiple subjects for the same content in the VideoSet can be well modeled by the normal distribution. Then, we design an SUR prediction method with video quality degradation features and masking features and use them to predict the first, second and the third JND points and their corresponding SUR curves. Finally, we verify the performance of the proposed SUR prediction method with different configurations on the VideoSet. The experimental results demonstrate that the proposed SUR prediction method achieves good performance in various resolutions with the mean absolute error (MAE) of the SUR smaller than 0.05 on average.},
author = {Wang, Haiqiang and Zhang, Xinfeng and Yang, Chao and Kuo, C. C.Jay},
booktitle = {2018 Picture Coding Symposium, PCS 2018 - Proceedings},
doi = {10.1109/PCS.2018.8456243},
isbn = {9781538641606},
keywords = {Just Noticeable Difference,Satisfied User Ratio,Video Quality Assessment},
title = {{Analysis and Prediction of JND-Based Video Quality Model}},
year = {2018}
}
@article{Lin2011,
abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
author = {Lin, Weisi and {Jay Kuo}, C. C.},
doi = {10.1016/j.jvcir.2011.01.005},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Common feature and artifact detection,Full reference,Human visual system (HVS),Just-noticeable distortion,No reference,Reduced reference,Signal decomposition,Signal-driven model,Vision-based model,Visual attention},
title = {{Perceptual visual quality metrics: A survey}},
year = {2011}
}
@article{Bosse2018,
abstract = {We present a deep neural network-based approach to image quality assessment (IQA). The network is trained end-to-end and comprises ten convolutional layers and five pooling layers for feature extraction, and two fully connected layers for regression, which makes it significantly deeper than related IQA models. Unique features of the proposed architecture are that: 1) with slight adaptations it can be used in a no-reference (NR) as well as in a full-reference (FR) IQA setting and 2) it allows for joint learning of local quality and local weights, i.e., relative importance of local quality to the global quality estimate, in an unified framework. Our approach is purely data-driven and does not rely on hand-crafted features or other types of prior domain knowledge about the human visual system or image statistics. We evaluate the proposed approach on the LIVE, CISQ, and TID2013 databases as well as the LIVE In the wild image quality challenge database and show superior performance to state-of-the-art NR and FR IQA methods. Finally, cross-database evaluation shows a high ability to generalize between different databases, indicating a high robustness of the learned features.},
archivePrefix = {arXiv},
arxivId = {1612.01697},
author = {Bosse, Sebastian and Maniry, Dominique and M{\"{u}}ller, Klaus Robert and Wiegand, Thomas and Samek, Wojciech},
doi = {10.1109/TIP.2017.2760518},
eprint = {1612.01697},
file = {:home/lego1st/Documents/School/DIPQA/ref/DNNs-for-NR-and-FR-IQA.pdf:pdf},
isbn = {9781509059669},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Full-reference image quality assessment,deep learning,feature extraction,neural networks,no-reference image quality assessment,quality pooling,regression},
number = {1},
pages = {206--219},
pmid = {24478645},
title = {{Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment}},
volume = {27},
year = {2018}
}
@article{Zhang2019,
author = {Zhang, Xinfeng and Lin, Weisi and Wang, Shiqi and Liu, Jiaying and Ma, Siwei and Gao, Wen},
doi = {10.1109/TIP.2018.2874283},
file = {:home/lego1st/Documents/School/DIPQA/ref/Fine-grained Assessment for Compressed Images.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Image quality assessment,fine-grained distortion levels,image database,peceptual image compression,subjective assessment},
number = {3},
pages = {1163--1175},
publisher = {IEEE},
title = {{Fine-Grained Quality Assessment for Compressed Images}},
volume = {28},
year = {2019}
}
@article{WenHeng;TingtingJiang2017,
author = {{Wen Heng; Tingting Jiang}},
file = {:home/lego1st/Documents/School/DIPQA/ref/Image-Patch.pdf:pdf},
isbn = {9781509041176},
journal = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2017},
pages = {1238--1242},
title = {{FROM IMAGE QUALITY TO PATCH QUALITY : AN IMAGE-PATCH MODEL FOR NO-REFERENCE IMAGE QUALITY ASSESSMENT Wen Heng and Tingting Jiang National Engineering Laboratory for Video Technology , Cooperative Medianet Innovation Center , School of Electronics Engineer}},
year = {2017}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:home/lego1st/Documents/School/DIPQA/ref/1409.1556.pdf:pdf},
pages = {1--14},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
