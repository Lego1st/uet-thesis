@inproceedings{Chen2010,
abstract = {Investigation on the human perception can play an important role in video signal processing. Recently, there has been great interest in incorporating the human perception in video coding systems to enhance the perceptual quality of the represented visual signal. However, the limited understanding of the human visual system and high complexity of computational models of human visual system make it a challenging task. Furthermore, the hybrid video coding structure brings difficulties to integrate computational models with coding components to fulfill the requirements. In this paper, we review the physiological characteristics of human perception and address the most relevant aspects to video coding applications. Moreover, we discuss the computational models and metrics which guide the design and implementation of the video coding system, as well as the recent advances in perceptual video coding. To introduce this overview with the latest technologies and most promising directions in perceptual video coding, we focus on three key areas. Specifically, we cover 1) visual attention and sensitivity modeling, with which we concentrate on the computational models of bottom-up and top-down attention, contrast sensitivity functions and masking effects, and fovea based manipulations; 2) perceptual quality optimization for constrained video coding, with which we discuss how to achieve maximum perceptual quality whilst satisfying various constraints; and 3) the impact of the human perception on advanced video applications, including emerging immersive multimedia services, and compression of high dynamic range video content and 3D video. For each aspect, we discuss the major challenges, highlight significant approaches, and outline future research directions.},
author = {Chen, Zhenzhong and Lin, Weisi and {Ngi Ngan}, King},
booktitle = {2010 IEEE International Conference on Multimedia and Expo, ICME 2010},
doi = {10.1109/ICME.2010.5582549},
isbn = {9781424474912},
keywords = {Human visual system,Perceptual video coding,Quality optimization,Visual perception},
title = {{Perceptual video coding: Challenges and approaches}},
year = {2010}
}
@article{Wang2018a,
abstract = {{\textcopyright} 1999-2012 IEEE. With the widespread adoption of multidevice communication, such as telecommuting, screen content images (SCIs) have become more closely and frequently related to our daily lives. For SCIs, the tasks of accurate visual quality assessment, high-efficiency compression, and suitable contrast enhancement have thus currently attracted increased attention. In particular, the quality evaluation of SCIs is important due to its good ability for instruction and optimization in various processing systems. Hence, in this paper, we develop a new objective metric for research on perceptual quality assessment of distorted SCIs. Compared to the classical MSE, our method, which mainly relies on simple convolution operators, first highlights the degradations in structures caused by different types of distortions and then detects salient areas where the distortions usually attract more attention. A comparison of our algorithm with the most popular and state-of-The-Art quality measures is performed on two new SCI databases (SIQAD and SCD). Extensive results are provided to verify the superiority and efficiency of the proposed IQA technique.},
author = {Wang, Shiqi and Gu, Ke and Zhang, Xinfeng and Lin, Weisi and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2016.2602764},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Image quality assessment (IQA),reduced reference (RR),screen content images (SCIs)},
title = {{Reduced-Reference Quality Assessment of Screen Content Images}},
year = {2018}
}
@inproceedings{Zhang2016,
abstract = {During the past few years, there have been various kinds of content-aware image retargeting methods proposed for image resizing. However, the lack of effective objective retargeting quality metric limits the further development of image retargeting. Different from the traditional image quality assessment, the quality degradation of the retargeted images is mainly caused by the geometric changes due to retargeting. In this paper, we propose a practical approach to reveal the geometric changes during image retargeting, and design an Aspect Ratio Similarity (ARS) metric to predict the visual quality of the retargeted image. The experimental results on the widely used dataset show that the proposed metric outperforms the state of the arts.},
author = {Zhang, Yabin and Lin, Weisi and Zhang, Xinfeng and Fang, Yuming and Li, Leida},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2016.7471842},
isbn = {9781479999880},
issn = {15206149},
keywords = {Image retargeting quality assessment,aspect ratio similarity},
title = {{Aspect Ratio Similarity (ARS) for image retargeting quality assessment}},
year = {2016}
}
@article{Huynh-Thu2008,
abstract = {Experimental data are presented that clearly demonstrate the scope of application of peak signal-to-noise ratio (PSNR) as a video quality metric. It is shown that as long as the video content and the codec type are not changed, PSNR is a valid quality measure. However, when the content is changed, correlation between subjective quality and PSNR is highly reduced. Hence PSNR cannot be a reliable method for assessing the video quality across different video contents},
author = {Huynh-Thu, Q. and Ghanbari, M.},
doi = {10.1049/el:20080522},
issn = {00135194},
journal = {Electronics Letters},
title = {{Scope of validity of PSNR in image/video quality assessment}},
year = {2008}
}
@article{Zhang2017,
author = {Zhang, Xinfeng and Wang, Shiqi and Gu, Ke and Lin, Weisi and Ma, Siwei and Gao, Wen},
doi = {10.1109/LSP.2016.2641456},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Image compression,just-noticeable difference (JND),quantization table,rate-distortion},
title = {{Just-Noticeable Difference-Based Perceptual Optimization for JPEG Compression}},
year = {2017}
}
@inproceedings{Ye2012,
abstract = {In this paper, we present an efficient general-purpose objective $\backslash$nno-reference (NR) image quality assessment (IQA) framework based on unsupervised $\backslash$nfeature learning. The goal is to build a computational model to automatically $\backslash$npredict human perceived image quality without a reference image and without $\backslash$nknowing the distortion present in the image. Previous approaches for this $\backslash$nproblem typically rely on hand-crafted features which are carefully designed $\backslash$nbased on prior knowledge. In contrast, we use raw-image-patches extracted from a $\backslash$nset of unlabeled images to learn a dictionary in an unsupervised manner. We use $\backslash$nsoft-assignment coding with max pooling to obtain effective image $\backslash$nrepresentations for quality estimation. The proposed algorithm is very $\backslash$ncomputationally appealing, using raw image patches as local descriptors and $\backslash$nusing soft-assignment for encoding. Furthermore, unlike previous methods, our $\backslash$nunsupervised feature learning strategy enables our method to adapt to different $\backslash$ndomains. CORNIA (Codebook Representation for No-Reference Image Assessment) is $\backslash$ntested on LIVE database and shown to perform statistically better than the $\backslash$nfull-reference quality measure, structural similarity index (SSIM) and is shown $\backslash$nto be comparable to state-of-the-art general purpose NR-IQA algorithms.},
author = {Ye, Peng and Kumar, Jayant and Kang, Le and Doermann, David},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247789},
isbn = {9781467312264},
issn = {10636919},
title = {{Unsupervised feature learning framework for no-reference image quality assessment}},
year = {2012}
}
@inproceedings{Kang2014,
abstract = {In this work we describe a Convolutional Neural Network (CNN) to accurately predict image quality without a reference image. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max and min pooling, two fully connected layers and an output node. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating image quality. This approach achieves state of the art performance on the LIVE dataset and shows excellent generalization ability in cross dataset experiments. Further experiments on images with local distortions demonstrate the local quality estimation ability of our CNN, which is rarely reported in previous literature.},
author = {Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.224},
isbn = {9781479951178},
issn = {10636919},
keywords = {Convolutional Neural Network,image quality assessment},
title = {{Convolutional neural networks for no-reference image quality assessment}},
year = {2014}
}
@inproceedings{Kang2015,
abstract = {The Iowa Center for Research on Botanical Dietary Supplements seeks$\backslash$nto optimize Echinacea, Hypericum, and Prunella botanical supplements$\backslash$nfor human-health benefit, emphasizing antiviral, anti-inflammatory$\backslash$nand anti-pain activities. This mini-review reports on ongoing studies$\backslash$non Hypericum. The Center uses the genetically diverse, well-documented$\backslash$nHypericum populations collected and maintained at the USDA-ARS North$\backslash$nCentral Regional Plant Introduction Station (NCRPIS), and the strength$\backslash$nof research in synthetic chemistry at Iowa State University to tap$\backslash$nnatural diversity, to help discover key constituents and interactions$\backslash$namong constituents that impact bioactivity and toxicity. The NCRPIS$\backslash$nhas acquired more than 180 distinct populations of Hypericum, with$\backslash$na focus on Hypericum perforatum L. (Hypericaceae), representing about$\backslash$n13{\%} of currently recognized taxa. Center chemists have developed$\backslash$nnovel synthetic pathways for key flavones, acyl phloroglucinols,$\backslash$nhyperolactones and a tetralin that have been found in Hypericum,$\backslash$nand these compounds are used as standards and for bioactivity studies.$\backslash$nBoth light-dependent and light-independent anti-viral activities$\backslash$nhave been identified by using bioactivity-guided fractionation of$\backslash$nH. perforatum and a HIV-1 infection test system. Our Center has focused$\backslash$non light-independent activity, potentially due to novel chemicals,$\backslash$nand polar fractions are undergoing further fractionation. Anti-inflammatory$\backslash$nactivity has been found to be light-independent, and fractionation$\backslash$nof a flavonoid-rich extract revealed four compounds (amentoflavone,$\backslash$nchlorogenic acid, pseudohypericin and quercetin) that interacted$\backslash$nin the light to inhibit lipopolysaccharide-induced prostaglandin$\backslash$nE(2) activity. The Center continues to explore novel populations$\backslash$nof H. perforatum and related species to identify constituents and$\backslash$ninteractions of constituents that contribute to potential health$\backslash$nbenefits related to infection.},
author = {Kang, Le and Ye, Peng and Li, Yi and Doermann, David},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2015.7351311},
isbn = {9781479983391},
issn = {15224880},
keywords = {CNN,Image quality assessment,image distortion classification,no-reference},
title = {{Simultaneous estimation of image quality and distortion via multi-task convolutional neural networks}},
year = {2015}
}
@article{Zhang2011,
abstract = {Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.},
author = {Zhang, Lin and Zhang, Lei and Mou, Xuanqin and Zhang, David},
doi = {10.1109/TIP.2011.2109730},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Gradient,image quality assessment (IQA),low-level feature,phase congruency (PC)},
title = {{FSIM: A feature similarity index for image quality assessment}},
year = {2011}
}
@inproceedings{Zhang2010,
abstract = {Image quality assessment (IQA) aims to provide computational models to measure the image quality in a perceptually consistent manner. In this paper, a novel feature based IQA model, namely Riesz-transform based Feature SIMilarity metric (RFSIM), is proposed based on the fact that the human vision system (HVS) perceives an image mainly according to its low-level features. The 1st-order and 2nd-order Riesz transform coefficients of the image are taken as image features, while a feature mask is defined as the edge locations of the image. The similarity index between the reference and distorted images is measured by comparing the two feature maps at key locations marked by the feature mask. Extensive experiments on the comprehensive TID2008 database indicate that the proposed RFSIM metric is more consistent with the subjective evaluation than all the other competing methods evaluated.},
author = {Zhang, Lin and Zhang, Lei and Mou, Xuanqin},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2010.5649275},
isbn = {9781424479948},
issn = {15224880},
keywords = {Image quality assessment,Monogenic signal,Riesz transform},
title = {{RFSIM: A feature based image quality assessment metric using Riesz transforms}},
year = {2010}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
title = {{Image quality assessment: From error visibility to structural similarity}},
year = {2004}
}
@inproceedings{Ma2016,
abstract = {In-loop filtering has emerged as an essential coding tool since H.264/AVC, due to its delicate design, which reduces different kinds of compression artifacts. However, existing in-loop filters rely only on local image correlations, largely ignoring nonlocal similarities. In this article, the authors explore the design philosophy of in-loop filters and discuss their vision for the future of in-loop filter research by examining the potential of nonlocal similarities. Specifically, the group-based sparse representation, which jointly exploits an image's local and nonlocal self-similarities, lays a novel and meaningful groundwork for in-loop filter design. Hard- and soft-thresholding filtering operations are applied to derive the sparse parameters that are appropriate for compression artifact reduction. Experimental results show that this in-loop filter design can significantly improve the compression performance of the High Efficiency Video Coding (HEVC) standard, leading us in a new direction for improving compression efficiency.},
author = {Ma, Siwei and Zhang, Xinfeng and Zhang, Jian and Jia, Chuanmin and Wang, Shiqi and Gao, Wen},
booktitle = {IEEE Multimedia},
doi = {10.1109/MMUL.2016.16},
issn = {1070986X},
keywords = {HEVC,graphics,in-loop filtering,multimedia,nonlocal similarity,sparse representation,video coding},
title = {{Nonlocal in-loop filter: The way toward next-generation video coding?}},
year = {2016}
}
@article{Zhang2017a,
abstract = {{\textcopyright} 2017 IEEE. In video coding, the in-loop filtering has emerged as a key module due to its significant improvement on compression performance since H.264/Advanced Video Coding. Existing incorporated in-loop filters in video coding standards mainly take advantage of the local smoothness prior model used for images. In this paper, we propose a novel adaptive loop filter utilizing image nonlocal prior knowledge by imposing the low-rank constraint on similar image patches for compression noise reduction. In the filtering process, the reconstructed frame is first divided into image patch groups according to image patch similarity. The proposed in-loop filtering is formulated as an optimization problem with low-rank constraint for every group of image patches independently. It can be efficiently solved by soft-thresholding singular values of the matrix composed of image patches in the same group. To adapt the properties of the input sequences and bit budget, an adaptive threshold derivation model is established for every group of image patches according to the characteristics of compressed image patches, quantization parameters, and coding modes. Moreover, frame-level and largest coding unit-level control flags are signaled to further improve the adaptability from the sense of rate-distortion optimization. The performance of the proposed in-loop filter is analyzed when it collaborates with the existing in-loop filters in High Efficiency Video Coding. Extensive experimental results show that our proposed in-loop filter can further improve the performance of state-of-the-art video coding standard significantly, with up to 16{\%} bit-rate savings.},
author = {Zhang, Xinfeng and Xiong, Ruiqin and Lin, Weisi and Zhang, Jian and Wang, Shiqi and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2016.2581618},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {High Efficiency Video Coding (HEVC),in-loop filter,nonlocal,singular value decomposition (SVD),video coding},
title = {{Low-Rank-Based Nonlocal Adaptive Loop Filter for High-Efficiency Video Compression}},
year = {2017}
}
@article{Wang2012,
abstract = {We propose a rate-distortion optimization (RDO) scheme based on the structural similarity (SSIM) index, which was found to be a better indicator of perceived image quality than mean-squared error, but has not been fully exploited in the context of image and video coding. At the frame level, an adaptive Lagrange multiplier selection method is proposed based on a novel reduced-reference statistical SSIM estimation algorithm and a rate model that combines the side information with the entropy of the transformed residuals. At the macroblock level, the Lagrange multiplier is further adjusted based on an information theoretical approach that takes into account both the motion information content and perceptual uncertainty of visual speed perception. Finally, the mode for H.264/AVC coding is selected by the SSIM index and the adjusted Lagrange multiplier. Extensive experiments show that the proposed scheme can achieve significantly better rate-SSIM performance and provide better visual quality than conventional RDO coding schemes.},
author = {Wang, Shiqi and Rehman, Abdul and Wang, Zhou and Ma, Siwei and Gao, Wen},
doi = {10.1109/TCSVT.2011.2168269},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {H.264/AVC coding,Lagrange multiplier,rate-distortion optimization,reduced-reference image quality assessment,structural similarity (SSIM) index},
title = {{SSIM-motivated rate-distortion optimization for video coding}},
year = {2012}
}
@article{Channappayya2008,
abstract = {In this paper, we derive bounds on the structural similarity (SSIM) index as a function of quantization rate for fixed-rate uniform quantization of image discrete cosine transform (DCT) coefficients under the high-rate assumption. The space domain SSIM index is first expressed in terms of the DCT coefficients of the space domain vectors. The transform domain SSIM index is then used to derive bounds on the average SSIM index as a function of quantization rate for uniform, Gaussian, and Laplacian sources. As an illustrative example, uniform quantization of the DCT coefficients of natural images is considered. We show that the SSIM index between the reference and quantized images fall within the bounds for a large set of natural images. Further, we show using a simple example that the proposed bounds could be very useful for rate allocation problems in practical image and video coding applications.},
author = {Channappayya, Sumohana S. and Bovik, Alan Conrad and Heath, Robert W.},
doi = {10.1109/TIP.2008.2001400},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
title = {{Rate bounds on SSIM index of quantized images}},
year = {2008}
}
@inproceedings{Wang2018,
abstract = {The just-noticeable-difference (JND) visual perception property has received much attention in characterizing human subjective viewing experience of compressed video. In this work, we quantify the JND-based video quality assessment model using the satisfied user ratio (SUR) curve, and show that the SUR model can be greatly simplified since the JND points of multiple subjects for the same content in the VideoSet can be well modeled by the normal distribution. Then, we design an SUR prediction method with video quality degradation features and masking features and use them to predict the first, second and the third JND points and their corresponding SUR curves. Finally, we verify the performance of the proposed SUR prediction method with different configurations on the VideoSet. The experimental results demonstrate that the proposed SUR prediction method achieves good performance in various resolutions with the mean absolute error (MAE) of the SUR smaller than 0.05 on average.},
author = {Wang, Haiqiang and Zhang, Xinfeng and Yang, Chao and Kuo, C. C.Jay},
booktitle = {2018 Picture Coding Symposium, PCS 2018 - Proceedings},
doi = {10.1109/PCS.2018.8456243},
isbn = {9781538641606},
keywords = {Just Noticeable Difference,Satisfied User Ratio,Video Quality Assessment},
title = {{Analysis and Prediction of JND-Based Video Quality Model}},
year = {2018}
}
@article{Lin2011,
abstract = {Visual quality evaluation has numerous uses in practice, and also plays a central role in shaping many visual processing algorithms and systems, as well as their implementation, optimization and testing. In this paper, we give a systematic, comprehensive and up-to-date review of perceptual visual quality metrics (PVQMs) to predict picture quality according to human perception. Several frequently used computational modules (building blocks of PVQMs) are discussed. These include signal decomposition, just-noticeable distortion, visual attention, and common feature and artifact detection. Afterwards, different types of existing PVQMs are presented, and further discussion is given toward feature pooling, viewing condition, computer-generated signal and visual attention. Six often-used image metrics (namely SSIM, VSNR, IFC, VIF, MSVD and PSNR) are also compared with seven public image databases (totally 3832 test images). We highlight the most significant research work for each topic and provide the links to the extensive relevant literature. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
author = {Lin, Weisi and {Jay Kuo}, C. C.},
doi = {10.1016/j.jvcir.2011.01.005},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Common feature and artifact detection,Full reference,Human visual system (HVS),Just-noticeable distortion,No reference,Reduced reference,Signal decomposition,Signal-driven model,Vision-based model,Visual attention},
title = {{Perceptual visual quality metrics: A survey}},
year = {2011}
}
@article{Bosse2018,
abstract = {We present a deep neural network-based approach to image quality assessment (IQA). The network is trained end-to-end and comprises ten convolutional layers and five pooling layers for feature extraction, and two fully connected layers for regression, which makes it significantly deeper than related IQA models. Unique features of the proposed architecture are that: 1) with slight adaptations it can be used in a no-reference (NR) as well as in a full-reference (FR) IQA setting and 2) it allows for joint learning of local quality and local weights, i.e., relative importance of local quality to the global quality estimate, in an unified framework. Our approach is purely data-driven and does not rely on hand-crafted features or other types of prior domain knowledge about the human visual system or image statistics. We evaluate the proposed approach on the LIVE, CISQ, and TID2013 databases as well as the LIVE In the wild image quality challenge database and show superior performance to state-of-the-art NR and FR IQA methods. Finally, cross-database evaluation shows a high ability to generalize between different databases, indicating a high robustness of the learned features.},
archivePrefix = {arXiv},
arxivId = {1612.01697},
author = {Bosse, Sebastian and Maniry, Dominique and M{\"{u}}ller, Klaus Robert and Wiegand, Thomas and Samek, Wojciech},
doi = {10.1109/TIP.2017.2760518},
eprint = {1612.01697},
file = {:home/lego1st/Documents/School/DIPQA/ref/DNNs-for-NR-and-FR-IQA.pdf:pdf},
isbn = {9781509059669},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Full-reference image quality assessment,deep learning,feature extraction,neural networks,no-reference image quality assessment,quality pooling,regression},
number = {1},
pages = {206--219},
pmid = {24478645},
title = {{Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment}},
volume = {27},
year = {2018}
}
@article{Zhang2019,
author = {Zhang, Xinfeng and Lin, Weisi and Wang, Shiqi and Liu, Jiaying and Ma, Siwei and Gao, Wen},
doi = {10.1109/TIP.2018.2874283},
file = {:home/lego1st/Documents/School/DIPQA/ref/Fine-grained Assessment for Compressed Images.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Image quality assessment,fine-grained distortion levels,image database,peceptual image compression,subjective assessment},
number = {3},
pages = {1163--1175},
publisher = {IEEE},
title = {{Fine-Grained Quality Assessment for Compressed Images}},
volume = {28},
year = {2019}
}
@article{WenHeng;TingtingJiang2017,
author = {{Wen Heng; Tingting Jiang}},
file = {:home/lego1st/Documents/School/DIPQA/ref/Image-Patch.pdf:pdf},
isbn = {9781509041176},
journal = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2017},
pages = {1238--1242},
title = {{FROM IMAGE QUALITY TO PATCH QUALITY : AN IMAGE-PATCH MODEL FOR NO-REFERENCE IMAGE QUALITY ASSESSMENT Wen Heng and Tingting Jiang National Engineering Laboratory for Video Technology , Cooperative Medianet Innovation Center , School of Electronics Engineer}},
year = {2017}
}
