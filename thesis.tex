\documentclass[a4paper, 13pt, oneside]{Thesis}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{csquotes}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{multirow}
\usepackage{subcaption}

\addto\captionsenglish{\renewcommand{\bibname}{References}}
%
\title{MODELING HUMAN VISUAL SYSTEM \\ IN PATCH-BASE IMAGE QUALITY ASSESSMENT \\ USING DEEP LEARNING}	%% title
\author{Nguyen Trung Nghia}			%% author's name

%
\newcommand{\argmax}{\arg\!\max}

\begin{document}
%
\maketitle

%
\chapter*{Authorship}

\emph{
\say{I hereby declare that the work contained in this thesis is of my own and has not been
previously submitted for a degree or diploma at this or any other higher education
institution. To the best of my knowledge and belief, the thesis contains no materials
previously published or written by another person except where due reference or
acknowledgement is made.}
}

\bigskip
Signature: .................

%
\chapter*{Supervisor’s approval}
\emph{
\say{I hereby approve that the thesis in its current form is ready for committee examination
as a requirement for the Bachelor of Computer Science degree at the University of
Engineering and Technology.}
}

\bigskip
Signature: .................


\chapter*{Acknowledgement}

First and foremost, I would like to express my sincere thanks to my supervisor Ph.D. Le Thanh Ha and M.Sc. Pham Thanh Tung for their support and guidance throughout this research work.

I greatly appreciated the Department of Computational Science and Engineering, and HMI Lab both at the VNU UET for the generous support.

I would also like to thanks my teachers and friends for their support throughout my time in
University of Engineering and Technology, Vietnam National University, Hanoi

This thesis was partly supported by the University of Fire Fighting and Prevention under the subjective image quality experiment and InfoRe Technology for providing working environment. 


\chapter*{Abstract}

As humans are the ultimate receivers of the majority of visual signals being processed, the most accurate way of assessing image quality is to ask humans for their opinions of an image’s quality, known as the subjective image quality assessment (IQA). The subjective image quality scores gathered from all subjects are processed to be the mean opinion score (MOS), which is regarded as the ground truth of image quality. Conventionally, a number of full-reference image quality assessment (FR-IQA) methods adopted various computational models of the human visual system (HVS) from psychological vision science research.

The image compression is one of the most
prominent applications that require IQA metrics to be highly
correlated with human vision. To explore IQA algorithms that are
more consistent with human vision, several calibrated databases
have been constructed. However, the distorted images in the existing databases are usually generated by corrupting the pristine
images with various distortions in coarse levels, such that the
IQA algorithms validated on them may be inefficient to optimize
the image compression with fine-grained quality
differences. In addition, HVS is differently sensitive to features of image patch, the \enquote*{ground
truth} quality of patch is essential for training patch-based methods, but in practice it’s easy to obtain the ground truth quality of images
rather than patches. 

So an experimental quality assessment to approach database for image patch has been developed. We propose Full-reference Deep Image-Patch Quality Assessment (DIPQA), a novel image-patch quality assessment that used deep neural network to estimate the \enquote*{ground-truth} for patches with the developed database.

Seven well-know IQA algorithms are evaluated and analyzed on the proposed database to show that there is still large room for improvement regarding fine-grained patch-based method. In the following experiment, we train and evaluate the proposed DIPQA on the proposed database and show competitive performance to other FR methods. DIPQA is expected to improve the performance of many applications that require patch's \enquote{ground truth} especially in image compression. 


%
\tableofcontents
\listoffigures
\listoftables

\listofsymbols{ll}  % Include a list of Abbreviations (a table of two columns)
{
  % \textbf{Acronym} & \textbf{W}hat (it) \textbf{S}tands \textbf{F}or \\
  %\textbf{LAH} & \textbf{L}ist \textbf{A}bbreviations \textbf{H}ere \\
  IQA & Image Quality Assessment \\
  CNN & Convolutional Neural Network \\
  DIPQA & Deep Image-Patch Quality Assessment \\
  DMOS & Differential Mean Opinion Score \\
  FR & Full-reference \\
  MOS & Mean Opinion Score \\
  NR & No-reference \\
  RR & Reduced-reference \\
}
%
\input{chapters/introduction}\newpage\cleardoublepage
\input{chapters/background}\newpage\cleardoublepage
\input{chapters/method}\newpage\cleardoublepage
\input{chapters/evaluation}\newpage\cleardoublepage
\input{chapters/conclusion}\newpage\cleardoublepage

\nocite{*}
\bibliography{references}\newpage\cleardoublepage
\bibliographystyle{plain}
\end{document}